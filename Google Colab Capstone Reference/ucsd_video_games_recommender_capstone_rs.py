# -*- coding: utf-8 -*-
"""UCSD Video Games Recommender Capstone - RS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k36DS0-Y-gwdZEVe7pmK1lG5IIdVIMLe

# Introduction:

This notebook will guide you through the process of building a basic recommender system for video games using the Steam dataset from Kaggle. We will leverage the machine learning concept such as cosine similarity to identify games that are similar to those a user has previously enjoyed. By the end of this tutorial, you will have a functional recommender system that can provide personalized game suggestions.

#Note:
This recommender system utilizes a simplified approach focusing on cosine similarity. There are more advanced techniques and algorithms that can be explored to enhance the system's accuracy and sophistication.

# Other Sources & References:
* https://www.kaggle.com/code/ugurokarapunar/game-recommendations-on-steam
* https://www.kaggle.com/code/vinayadatiya/game-recommendation-system
* https://www.kaggle.com/code/dongminkimm/steam-game-recommendations
* https://www.kaggle.com/code/dameertaheemgay/real-machine-learning-project-2
* https://www.kaggle.com/code/thakursankalp/steam-game-recommendation-engine
* https://www.kaggle.com/code/auradee/video-games-recommendation-system
* https://www.kaggle.com/code/jwyang91/steam-game-recommender
* https://www.kaggle.com/code/calven22/steam-recommender-system
* https://www.kaggle.com/code/mohamada2274/steam-video-games-recommendation
* https://www.kaggle.com/code/stpeteishii/steam-video-recommendation-lightfm
* https://www.kaggle.com/code/a3amat02/best-video-games-eda-and-game-recommendation
* https://www.kaggle.com/code/abhishekmanish/steam-games-dataset-eda-and-recommendation

# Import Modules

This section imports essential Python libraries for data manipulation, analysis, and machine learning. Pandas and NumPy enable efficient data handling and numerical computations. Libraries like mlxtend, sklearn, and scipy provide tools for frequent pattern mining, text vectorization, similarity calculations, and neighborhood-based methods used in the recommender system development. KaggleHub facilitates dataset retrieval from Kaggle directly within the Colab environment.
"""

# Install necessary packages
!pip install annoy
!pip install dask[dataframe]
!pip install implicit

# Import libraries
import dask.dataframe as dd
import gc
import implicit
import kagglehub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd

from annoy import AnnoyIndex
from dask import delayed
from mlxtend.frequent_patterns import apriori, association_rules
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LinearRegression
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import linear_kernel
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import csr_matrix
from scipy.sparse import coo_matrix

"""# Upload your Datasets

This project utilizes Steam datasets from Kaggle, varying in size and scope, to build a robust recommender system. These datasets contain information on games, users, recommendations, and ratings. Links to the Kaggle datasets are provided in the notebook for easy access. Ensure data is appropriately read into Pandas DataFrames for further processing.


* Size M: https://www.kaggle.com/datasets/antonkozyriev/game-recommendations-on-steam/data
* Size L: https://www.kaggle.com/datasets/whigmalwhim/steam-releases/data
* Size XL: https://www.kaggle.com/datasets/maestrocor/steam-games-and-user-playtime-data/data
* Size XL: https://www.kaggle.com/datasets/praffulsingh009/steam-video-games-2024


#Note
After multiple itteration, these dataset will not be used. But I would like to still keep them here for reference.
* https://www.kaggle.com/datasets/nikdavis/steam-store-games/data
* https://www.kaggle.com/datasets/shpadoinkle/steam-game-recommendation-ratings
* https://www.kaggle.com/datasets/mohamedtarek01234/steam-games-reviews-and-rankings?select=steam_game_reviews.csv
* https://www.kaggle.com/datasets/marcosparrasmolt/steam-library-metadata
* https://www.kaggle.com/datasets/bayuabdurrosyidyeye/steam-game-review?select=recommendations.csv
* https://www.kaggle.com/datasets/thedevastator/steam-games-user-statistics-and-features
* https://www.kaggle.com/datasets/mexwell/steamgames/data

"""

# Download CSV from Kaggle
path_1 = kagglehub.dataset_download("antonkozyriev/game-recommendations-on-steam")
path_2 = kagglehub.dataset_download("whigmalwhim/steam-releases")
path_3 = kagglehub.dataset_download("maestrocor/steam-games-and-user-playtime-data")
path_4 = kagglehub.dataset_download("praffulsingh009/steam-video-games-2024")

# List files in the downloaded directory
file_1 = os.listdir(path_1)
file_2 = os.listdir(path_2)
file_3 = os.listdir(path_3)
file_4 = os.listdir(path_4)
print("Files in the directory:", file_1)
print("Files in the directory:", file_2)
print("Files in the directory:", file_3)
print("Files in the directory:", file_4)

"""# DataFrames

This section focuses on loading the selected datasets into Pandas DataFrames, employing efficient memory management techniques. Functions like reduce_memory and data_generator are introduced to optimize memory usage, particularly for larger datasets. Each dataset is read into a separate DataFrame providing the foundation for subsequent data analysis and model building.
* df_games_1
* df_games_2
* df_games_3
* df_users
* df_recommendations
"""

# Function to reduce the memory usage of a DataFrame.
def reduce_memory(df):
    for col in df.columns:
        if df[col].dtype == 'float64':
            # Change to float32
            df[col] = df[col].astype('float32')
        if df[col].dtype == 'int64':
            # Change to int32
            df[col] = df[col].astype('int32')
        if df[col].dtype == 'object':
            # Exclude 'title' and 'genres'
            if col not in ['title', 'genres']:
                # Change to category
                df[col] = df[col].astype('category')
    return df

# Generator function to load data in chunks.
def data_generator(df, chunksize=10000):
    for i in range(0, df.shape[0], chunksize):
        yield df.iloc[i:i+chunksize]

# Now read the CSV file with the updated path
df_games_1 = reduce_memory(pd.read_csv(os.path.join(path_1, 'games.csv')))
df_games_2 = reduce_memory(pd.read_csv(os.path.join(path_2, 'game_data_all.csv')))
df_games_3 = reduce_memory(pd.read_csv(os.path.join(path_4, 'Steam Games 2024.csv')))
df_users = reduce_memory(pd.read_csv(os.path.join(path_3, 'user_game_played_data.csv')))
df_recommendations = reduce_memory(pd.read_csv(os.path.join(path_1, 'recommendations.csv')))

"""# Data Analysis

This section performs preliminary data exploration to gain insights into the structure and characteristics of the datasets. It includes examining the shape (number of rows and columns), data types of each column, and overall information using functions like shape, dtypes, and info. Displaying the first few rows of each DataFrame using head() allows for a quick overview of the data's content, aiding in understanding its composition and potential relevance for building the recommender system. This initial analysis informs subsequent steps in data preprocessing and model development.
"""

# Analysize shape
print(df_games_1.shape)
print(df_games_2.shape)
print(df_games_3.shape)
print(df_users.shape)
print(df_recommendations.shape)

# Analysize dtypes
print(df_games_1.dtypes)
print(df_games_2.dtypes)
print(df_games_3.dtypes)
print(df_users.dtypes)
print(df_recommendations.dtypes)

# Analysize info
print(df_games_1.info())
print(df_games_2.info())
print(df_games_3.info())
print(df_users.info())
print(df_recommendations.info())

# Analyze columns
print(df_games_1.columns)
print(df_games_2.columns)
print(df_games_3.columns)
print(df_users.columns)
print(df_recommendations.columns)

"""TT# Feature Engineering

The feature engineering process involved several crucial steps to prepare the data for the recommender system. First, irrelevant columns were removed from the datasets to streamline the data. Then, columns were renamed and reordered for consistency and to adjust data types. Multiple datasets (games.csv, steamgames.csv, and others) were strategically merged to create a comprehensive dataset combining game details with user information and play times. Finally, invalid and missing values were addressed to improve data quality. This thorough feature engineering ensured that the recommender system had access to essential information represented in a format suitable for model training.


"""

# Remove columns that are not needed
# Get a list of existing columns
existing_columns_df_games_1 = df_games_1.columns
existing_columns_df_games_2 = df_games_2.columns
existing_columns_df_games_3 = df_games_3.columns
existing_columns_df_recommendations = df_recommendations.columns

# Columns to drop
columns_to_drop_df_games_1 = ['date_release', 'win', 'mac', 'linux', 'positive_ratio',	'user_reviews', 'price_final', 'price_original',
                              'discount', 'steam_deck']
columns_to_drop_df_games_2 = ['Unnamed: 0', 'release', 'peak_players', 'positive_reviews', 'negative_reviews', 'total_reviews',
                              'primary_genre', 'store_genres', 'publisher', 'developer', 'detected_technologies', 'store_asset_mod_time',
                              'review_percentage', 'players_right_now', '24_hour_peak', 'all_time_peak', 'all_time_peak_date']
columns_to_drop_df_games_3 = ['Release date','Estimated owners', 'Peak CCU', 'Required age', 'Price', 'Discount', 'DLC count', 'About the game',
                              'Supported languages', 'Full audio languages', 'Reviews', 'Header image', 'Website', 'Support url',
                              'Support email', 'Windows', 'Mac', 'Linux', 'Metacritic url', 'User score', 'Positive', 'Negative',
                              'Score rank', 'Achievements', 'Recommendations', 'Notes', 'Average playtime forever',
                              'Average playtime two weeks', 'Median playtime forever', 'Median playtime two weeks', 'Developers',
                              'Publishers', 'Categories', 'Tags', 'Screenshots', 'Movies']
columns_to_drop_df_recommendations = ['helpful', 'funny', 'date', 'review_id']


# Find the common columns between existing and columns to drop
common_columns_df_games_1 = list(set(existing_columns_df_games_1) & set(columns_to_drop_df_games_1))
common_columns_df_games_2 = list(set(existing_columns_df_games_2) & set(columns_to_drop_df_games_2))
common_columns_df_games_3 = list(set(existing_columns_df_games_3) & set(columns_to_drop_df_games_3))
common_columns_df_recommendations = list(set(existing_columns_df_recommendations) & set(columns_to_drop_df_recommendations))

# Drop only the common columns
df_games_1 = df_games_1.drop(columns=common_columns_df_games_1,axis=1)
df_games_2 = df_games_2.drop(columns=common_columns_df_games_2,axis=1)
df_games_3 = df_games_3.drop(columns=common_columns_df_games_3,axis=1)
df_recommendations = df_recommendations.drop(columns=common_columns_df_recommendations,axis=1)

# Print the remaining columns to verify
print(df_games_1.columns)
print(df_games_2.columns)
print(df_games_3.columns)
print(df_users.columns)
print(df_recommendations.columns)

# Rename column to be consistent
df_games_2.rename(columns={'game': 'title', 'link': 'app_id', 'rating': 'rating_score'}, inplace=True)
df_games_3.rename(columns={'AppID': 'app_id', 'Name': 'title', 'Metacritic score': 'metacritic_score', 'Genres': 'genres'}, inplace=True)
df_users.rename(columns={'game_id': 'app_id', 'playtime_forever': 'total_playtime'}, inplace=True)
df_recommendations.rename(columns={'is_recommended': 'recommended', 'hours': 'total_hours'}, inplace=True)

# remove character from column
df_games_2['app_id'] = df_games_2['app_id'].str.replace(r'/app/','', regex=True).replace(r'/','', regex=True)

# reorder column
df_games_2 = df_games_2[['app_id', 'title', 'rating_score']]
df_games_3 = df_games_3[['app_id', 'title', 'genres', 'metacritic_score']]
df_users = df_users[['user_id', 'app_id', 'total_playtime']]
df_recommendations = df_recommendations[['user_id', 'app_id', 'recommended', 'total_hours']]

# convert app_id from df_games_2 to an int32
df_games_2['app_id'] = df_games_2['app_id'].astype('int32')

# Merge df_games_1 and df_games_2 dataframes to df_games_f1
df_games_f1 = pd.merge(
    df_games_1[['app_id', 'title', 'rating']],
    df_games_2[['app_id', 'title', 'rating_score']],
    on=['app_id', 'title'],  # Merge based on app_id and title
    how='inner'  # Use inner join to keep only matching rows
)

# Analyze shape
print(df_games_1.shape)
print(df_games_2.shape)
print(df_games_f1.shape)

# Merge df_games_f1 and df_games_3 dataframes to df_games_f2
df_games = pd.merge(
    df_games_f1[['app_id', 'title', 'rating', 'rating_score']],
    df_games_3[['app_id', 'title', 'genres', 'metacritic_score']],
    on=['app_id', 'title'],  # Merge based on app_id and title
    how='inner'  # Use inner join to keep only matching rows
)

# Analyze shape
print(df_games_f1.shape)
print(df_games_3.shape)
print(df_games.shape)

# Check any NaN/invalid value
print(df_games.isnull().values.any())
print(df_users.isnull().values.any())
print(df_recommendations.isnull().values.any())

# Which columns have null values?
print(df_games.columns[df_games.isna().any()].tolist())
print(df_users.columns[df_users.isna().any()].tolist())
print(df_recommendations.columns[df_recommendations.isna().any()].tolist())

# How many null values per column
# Count the missing values in each column
print(df_games.isnull().sum())
print(df_users.isnull().sum())
print(df_recommendations.isnull().sum())

# remove all user_id that have total_playtime equals zero
df_users = df_users[df_users['total_playtime'] != 0]
df_users.shape

# Merge df_games and df_users dataframes
df_games_users_all = pd.merge(df_games[['app_id', 'title', 'rating', 'rating_score']], df_users, on='app_id')
df_games_users_all = df_games_users_all[['user_id', 'title', 'rating', 'rating_score', 'total_playtime']]

print(df_games_users_all.head())

# Show Top 40 Games with the Highest Metacritic Score
df_games = df_games.sort_values(by='metacritic_score', ascending=False)

# Convert 'metacritic_score' to numeric, handling errors
df_games['metacritic_score'] = pd.to_numeric(df_games['metacritic_score'], errors='coerce')

# Show the plot
df_games.head(40).plot(kind='bar', x='title', y='metacritic_score') # This plot is ok with "title" on the x-axis (categorical)
plt.title('Top 40 Games with The Highest Metacritic Score')
plt.xlabel('Game Title')
plt.ylabel('Metacritic Score')
plt.xticks(rotation=90) # Rotate x-axis labels for better readability
plt.show()

# switch the presentation so that title on y axis and metacritic score on x axis
df_games_top40 = df_games.head(40).sort_values(by=['metacritic_score']) #sorting metacritic_score to have meaningful x axis tick labels
plt.figure(figsize=(10, 10)) #Adjust figure size if titles are cut off
plt.barh(df_games_top40['title'], df_games_top40['metacritic_score']) #Using plt.barh for horizontal bar plot
plt.title('Top 40 Games with The Highest Metacritic Score')
plt.xlabel('Metacritic Score')
plt.ylabel('Game Title')
plt.show()

# Show Top 30 Games with the Highest Number of Rating Received
df_games_users_all_count = df_games_users_all.groupby('title').size().reset_index(name='count')
df_games_users_all_count = df_games_users_all_count.sort_values(by='count', ascending=False)

# Show the plot
df_games_users_all_count.head(30).plot(kind='bar', x='title', y='count')
plt.title('Top 30 Games with The Highest Number of Rating Received')
plt.xlabel('Game Title')
plt.ylabel('Number of Rating Received')
plt.show()

# switch the presentation so that title on y axis and number of rating received on x axis
df_games_users_all_count_top30 = df_games_users_all_count.head(30).sort_values(by=['count'])
plt.figure(figsize=(10, 10))
plt.barh(df_games_users_all_count_top30['title'], df_games_users_all_count_top30['count'])
plt.title('Top 30 Games with The Highest Number of Rating Received')
plt.xlabel('Number of Rating Received')
plt.ylabel('Game Title')
plt.show()

# Show Top 30 Games with the Highest Number of Playing Time

# Combine total_playtime for each title
df_games_users_all_playtime = df_games_users_all.groupby('title')['total_playtime'].sum().reset_index() # Added parentheses here

# Sort based on the highest total_playtime
df_games_users_all_playtime = df_games_users_all_playtime.sort_values(by='total_playtime', ascending=False)

# Show the plot
df_games_users_all_playtime.head(30).plot(kind='bar', x='title', y='total_playtime')
plt.title('Top 30 Games with The Highest Number of Playing Time')
plt.xlabel('Game Title')
plt.ylabel('Total Playing Time')
plt.show()

# switch the presentation so that title on y axis and total playing time on x axis
df_games_users_all_playtime_top30 = df_games_users_all_playtime.head(30).sort_values(by=['total_playtime'])
plt.figure(figsize=(10, 10))
plt.barh(df_games_users_all_playtime_top30['title'], df_games_users_all_playtime_top30['total_playtime'])
plt.title('Top 30 Games with The Highest Number of Playing Time')
plt.xlabel('Total Playing Time')
plt.ylabel('Game Title')
plt.show()

"""# Recommendation #1 - Content Base Filtering based on 1 category (title)

The recommendation model utilizes a content-based filtering approach to suggest video games similar to a given game. It begins by creating a TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer to transform the game titles into numerical vectors, effectively capturing the importance of words within the titles. The model then calculates the cosine similarity matrix, which measures the similarity between these vectors. By creating indices of game names, the model can efficiently retrieve the index of the input game and compute its similarity scores with all other games. The similarity scores are sorted, and the top 10 games with the highest scores are selected as recommendations. This approach ensures that the recommended games share similar attributes with the input game, providing users with personalized and relevant suggestions.
"""

def recommendations(dataframe, title_col, game_name):
    # Create the TF-IDF vectorizer and transform the titles in the dataframe
    tfidf = TfidfVectorizer(stop_words="english")
    tfidf_matrix = tfidf.fit_transform(dataframe[title_col])

    # Calculate the cosine similarity matrix
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

    # Create indices of game names in the database
    indices = pd.Series(dataframe.index, index=dataframe[title_col]).drop_duplicates()

    # Get the index of the input game
    idx = indices[game_name]

    # Get the similarity scores of the game
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the similarity scores by game name
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the top 10 games with the highest similarity scores
    sim_scores = sim_scores[1:11]

    # Get the game indices according to the scores
    game_indices = [i[0] for i in sim_scores]

    # Return the relevant games
    return dataframe['title'].iloc[game_indices]

# Call the function
recommendations_list = recommendations(df_games, "title", "Grand Theft Auto V")
print(recommendations_list)

"""# Recommendation #2 - Content Base Filtering based on 2 categories (genres and rating_score)

The content-based recommendation model leverages both genres and rating scores to suggest video games similar to a given title. Initially, the model converts the genres into a TF-IDF (Term Frequency-Inverse Document Frequency) matrix, which captures the importance of genre terms within the dataset. It then computes the cosine similarity between these genre vectors. Simultaneously, the rating scores are normalized to ensure they are on a comparable scale. The model combines the genre similarity and normalized rating scores using specified weights, creating a comprehensive similarity matrix. By identifying the index of the target game and calculating its similarity scores with other games, the model generates a list of recommendations. These recommendations are filtered to exclude duplicates and games already rated by the user, and are sorted by similarity score to present the top suggestions. This approach ensures that the recommended games share similar genres and rating profiles with the input game, providing users with personalized and relevant suggestions.

# Note: Must use Kaggle due to RAM limitation
"""

# NOTE: Google Colab will not have enough RAM to run this - use Kaggle
# Content based recommendation based on Genres and Rating Score
def content_based_recommendation(game_title, df, genre_weight=0.5, rating_score_weight=0.5, similarity_threshold=0.6):
    # Convert 'genres' to string before applying TF-IDF
    df['genres'] = df['genres'].astype(str)

    # Create a TF-IDF matrix for genres
    tfidf_genres = TfidfVectorizer(stop_words='english')
    tfidf_matrix_genres = tfidf_genres.fit_transform(df['genres'])

    # Compute cosine similarity for genres
    cosine_sim_genres = cosine_similarity(tfidf_matrix_genres, tfidf_matrix_genres)

    # Normalize the rating_score
    df['rating_score_norm'] = (df['rating_score'] - df['rating_score'].min()) / (df['rating_score'].max() - df['rating_score'].min())

    # Combine similarities with weights
    similarity_matrix = (cosine_sim_genres * genre_weight) + (df['rating_score_norm'].values.reshape(-1, 1) * rating_score_weight)

    # Get the index of the target game title
    idx = df[df['title'] == game_title].index
    if len(idx) == 0:
        print(f"Game title '{game_title}' not found in the dataset.")
        return []
    else:
        idx = idx[0]

    # Remove duplicates and games already rated by the user
    rated_games = set([game_title])
    recommendations = []
    for i, score in enumerate(similarity_matrix[idx]):  # Iterate through the similarity scores
        if i != idx and df['title'].iloc[i] not in rated_games and score > similarity_threshold:
            recommendations.append((df['title'].iloc[i], score))
            rated_games.add(df['title'].iloc[i])

    # Sort recommendations by similarity score
    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)[:10]  # Top 5 recommendations

    return recommendations

# Test the recommendation engine (assuming df_games is defined with 'genres' and 'rating_score' columns)
game_titles = ['Grand Theft Auto V']
for game_title in game_titles:
    print(f"Recommendations for game title '{game_title}':")
    recommendations = content_based_recommendation(game_title, df_games)
    for title, score in recommendations:
        print(f"- {title} (Similarity Score: {score:.2f})")

"""# Recommendation #3 - Content Base Filtering based on 3 categories with Annoy

The content-based recommendation model leverages genres, rating scores, and Metacritic scores to suggest video games similar to a given title. Initially, the model converts the genres into a TF-IDF (Term Frequency-Inverse Document Frequency) matrix, capturing the importance of genre terms within the dataset. It then normalizes the rating and Metacritic scores to ensure they are on a comparable scale. These features are combined into a single matrix, which is used to create an Annoy (Approximate Nearest Neighbors Oh Yeah) index. Annoy efficiently handles large, sparse vector spaces, making it ideal for this application. By identifying the index of the target game and calculating its similarity scores with other games, the model generates a list of recommendations. These recommendations are filtered to exclude the target game and those with similarity scores below a specified threshold, ensuring a diverse and relevant selection. This approach integrates multiple aspects of game attributes, providing users with personalized and accurate suggestions.
"""

# Content-Based Recommendation based on genres, rating score, and metacritic score with Annoy
def content_based_recommendation(game_title, df, genre_weight=0.3, rating_score_weight=0.3, metacritic_score_weight=0.4, similarity_threshold=0.6):
    # Convert 'genres' to string before applying TF-IDF
    df['genres'] = df['genres'].astype(str)

    # Create a TF-IDF matrix for genres
    tfidf_genres = TfidfVectorizer(stop_words='english')
    tfidf_matrix_genres = tfidf_genres.fit_transform(df['genres'])

    # Convert 'rating_score' and 'metacritic_score' to numeric and normalize
    df['rating_score'] = pd.to_numeric(df['rating_score'], errors='coerce')
    df['metacritic_score'] = pd.to_numeric(df['metacritic_score'], errors='coerce')

    df['rating_score_norm'] = (df['rating_score'] - df['rating_score'].min()) / (df['rating_score'].max() - df['rating_score'].min())
    df['metacritic_score_norm'] = (df['metacritic_score'] - df['metacritic_score'].min()) / (df['metacritic_score'].max() - df['metacritic_score'].min())

    # Combine features into a single matrix
    combined_features = np.hstack([
        tfidf_matrix_genres.toarray() * genre_weight,
        df['rating_score_norm'].values.reshape(-1, 1) * rating_score_weight,
        df['metacritic_score_norm'].values.reshape(-1, 1) * metacritic_score_weight
    ])

    # Create Annoy index
    f = combined_features.shape[1]
    t = AnnoyIndex(f, 'angular')

    for i in range(len(combined_features)):
        t.add_item(i, combined_features[i])

    t.build(10)  # 10 trees

    # Get the index of the target game title
    idx = df[df['title'] == game_title].index
    if len(idx) == 0:
        print(f"Game title '{game_title}' not found in the dataset.")
        return []
    else:
        idx = idx[0]

    # Get similar items
    similar_items = t.get_nns_by_item(idx, 11, include_distances=True)

    recommendations = [(df['title'].iloc[i], 1 - dist) for i, dist in zip(similar_items[0], similar_items[1]) if i != idx and 1 - dist > similarity_threshold]

    return recommendations

# Test the recommendation engine (assuming df_games is defined with 'genres', 'rating_score', and 'metacritic_score' columns)
game_titles = ['Grand Theft Auto V']
for game_title in game_titles:
    print(f"Recommendations for game title '{game_title}':")
    recommendations = content_based_recommendation(game_title, df_games)
    for title, score in recommendations:
        print(f"- {title} (Similarity Score: {score:.2f})")

"""# Recommendation #4 - Content Base Filtering based on 4 categories with Annoy

This content-based filtering recommendation system leverages game features like genres, user ratings, rating scores, and Metacritic scores to suggest similar games. It employs TF-IDF to represent game genres numerically and normalizes rating scores and Metacritic scores for comparability. User ratings are encoded and normalized as well. These features are combined into a single vector for each game. Annoy, an approximate nearest neighbor search library, is used to efficiently find games with similar feature vectors, providing recommendations based on content similarity to the target game. This approach offers personalized suggestions by focusing on inherent game attributes that align with user preferences.
"""

# Content-Based Recommendation based on genres, rating, rating score, and metacritic score with Annoy
def content_based_recommendation(game_title, df, genre_weight=0.25, rating_weight=0.25, rating_score_weight=0.25, metacritic_score_weight=0.25, similarity_threshold=0.6):
    # Convert 'genres' to string before applying TF-IDF
    df['genres'] = df['genres'].astype(str)

    # Create a TF-IDF matrix for genres
    tfidf_genres = TfidfVectorizer(stop_words='english')
    tfidf_matrix_genres = tfidf_genres.fit_transform(df['genres'])

    # Convert 'rating_score' and 'metacritic_score' to numeric and normalize
    df['rating_score'] = pd.to_numeric(df['rating_score'], errors='coerce')
    df['metacritic_score'] = pd.to_numeric(df['metacritic_score'], errors='coerce')

    df['rating_score_norm'] = (df['rating_score'] - df['rating_score'].min()) / (df['rating_score'].max() - df['rating_score'].min())
    df['metacritic_score_norm'] = (df['metacritic_score'] - df['metacritic_score'].min()) / (df['metacritic_score'].max() - df['metacritic_score'].min())

    # --- Incorporating 'rating' ---
    # 1. Convert 'rating' to numerical representation using Label Encoding
    le = LabelEncoder()
    df['rating_encoded'] = le.fit_transform(df['rating'])

    # 2. Normalize the encoded rating
    df['rating_encoded_norm'] = (df['rating_encoded'] - df['rating_encoded'].min()) / (df['rating_encoded'].max() - df['rating_encoded'].min())

    # Combine features into a single matrix, including the encoded rating
    combined_features = np.hstack([
        tfidf_matrix_genres.toarray() * genre_weight,
        df['rating_encoded_norm'].values.reshape(-1, 1) * rating_weight,  # Added encoded rating
        df['rating_score_norm'].values.reshape(-1, 1) * rating_score_weight,
        df['metacritic_score_norm'].values.reshape(-1, 1) * metacritic_score_weight
    ])

    # Create Annoy index
    f = combined_features.shape[1]
    t = AnnoyIndex(f, 'angular')

    for i in range(len(combined_features)):
        t.add_item(i, combined_features[i])

    t.build(10)  # 10 trees

    # Get the index of the target game title
    idx = df[df['title'] == game_title].index
    if len(idx) == 0:
        print(f"Game title '{game_title}' not found in the dataset.")
        return []
    else:
        idx = idx[0]

    # Get similar items
    similar_items = t.get_nns_by_item(idx, 11, include_distances=True)

    recommendations = [(df['title'].iloc[i], 1 - dist) for i, dist in zip(similar_items[0], similar_items[1]) if i != idx and 1 - dist > similarity_threshold]

    return recommendations

# Test the recommendation engine (assuming df_games is defined with 'genres', 'rating_score', and 'metacritic_score' columns)
game_titles = ['Grand Theft Auto V']
for game_title in game_titles:
    print(f"Recommendations for game title '{game_title}':")
    recommendations = content_based_recommendation(game_title, df_games)
    for title, score in recommendations:
        print(f"- {title} (Similarity Score: {score:.2f})")

"""# Recommendation #5 - Collaborative Filtering based on 1 category (total_playtime)

The collaborative filtering recommendation model leverages user-item interactions to suggest video games. It begins by creating a user-item matrix, where each cell represents the total playtime of a user for a specific game. Missing values are filled with zeros to indicate no interaction. The model then calculates the cosine similarity between users, generating a user-user similarity matrix. By identifying the top N most similar users to the target user, the model can recommend games that these similar users have enjoyed. The recommendations are filtered to remove duplicates and limited to the top 20 suggestions. This approach ensures that the recommended games are based on the preferences and behaviors of users with similar tastes, providing personalized and relevant suggestions.
"""

# Collaborative Filtering using User-Item Interactions
def collaborative_filtering_recommendation(user_id, df):
  # Create the user-item matrix
  user_item_matrix = pd.pivot_table(df, values='total_playtime', index='user_id', columns='title', fill_value=0)
  print(user_item_matrix)

  # Fill missing values with 0 (indicating no rating)
  user_item_matrix = user_item_matrix.fillna(0)
  print(user_item_matrix)

  # Calculate user-user similarity matrix using cosine similarity
  user_similarity = cosine_similarity(user_item_matrix)
  print(user_similarity)

  # Get the similarity scores of the target user with all other users
  user_similarity_scores = user_similarity[user_id]
  print(user_similarity_scores)

  # Find the top N most similar users (excluding the target user)
  N = 5
  top_similar_users = np.argsort(user_similarity_scores)[::-1][1:N+1]
  print(top_similar_users)

  # Generate movie recommendations based on the most similar users
  recommendations = []
  for user in top_similar_users:
    recommendations.extend(df[df['user_id'] == user]['title'].values)
  print(recommendations)

  # Remove duplicates from recommendations
  recommendations = list(set(recommendations))

  # Limit to 20 recommendations
  recommendations = recommendations[:20]

  return recommendations

# Test the recommendation engines
user_ids = [1, 2, 3]
for user_id in user_ids:
    print(f"Recommendations for user {user_id}:")
    recommendations = collaborative_filtering_recommendation(user_id, df_games_users_all)
    for i, recommendation in enumerate(recommendations, 1):
        print(f"{i}. {recommendation}")

"""# Recommendation #6 - Collaborative Filtering based on 3 categories (rating, rating_score and total_playtime)

The collaborative filtering recommendation model utilizes user-item interactions across multiple categories, specifically rating and total playtime, to suggest video games. Initially, the model maps string-based ratings to numerical scores and normalizes both the rating scores and total playtime. These normalized values are combined into a single score, which is used to create a user-item matrix. By calculating the cosine similarity between users, the model generates a user-user similarity matrix. The similarity scores of the target user with all other users are then used to identify the top N most similar users. Recommendations are generated based on the games enjoyed by these similar users, ensuring a diverse and personalized selection. This approach effectively integrates multiple aspects of user preferences, providing robust and accurate game recommendations.
"""

# Collaborative Filtering using User-Item Interactions with multiple categories (rating and total_playtime)
def collaborative_filtering_recommendation(user_id, df):
    # Define a mapping for string ratings to numerical scores
    rating_mapping = {
        'Overwhelmingly Positive': 5,
        'Very Positive': 4,
        'Positive': 3,
        'Mostly Positive': 2,
        'Mixed': 1,
        'Mostly Negative': 0,
        'Negative': -1,
        'Very Negative': -2,
        'Overwhelmingly Negative': -3
    }

    # Map string ratings to numerical scores
    df['rating_score'] = df['rating'].map(rating_mapping)

    # Handle cases where the rating is not in the mapping (optional)
    df['rating_score'] = pd.to_numeric(df['rating_score'], errors='coerce')
    df['rating_score'] = df['rating_score'].fillna(df['rating_score'].mean())  # Or any suitable strategy

    # Normalize 'total_playtime' and 'rating_score'
    df['total_playtime_norm'] = (df['total_playtime'] - df['total_playtime'].min()) / (df['total_playtime'].max() - df['total_playtime'].min())
    df['rating_score_norm'] = (df['rating_score'] - df['rating_score'].min()) / (df['rating_score'].max() - df['rating_score'].min())

    # Create a combined score
    df['combined_score'] = df['total_playtime_norm'] * 0.5 + df['rating_score_norm'] * 0.5

    # Create the user-item matrix using the combined score
    user_item_matrix = pd.pivot_table(df, values='combined_score', index='user_id', columns='title', fill_value=0)

    # Calculate user-user similarity matrix using cosine similarity
    user_similarity = cosine_similarity(user_item_matrix)

    # Get the similarity scores of the target user with all other users
    user_similarity_scores = user_similarity[user_id - 1]  # Adjust index if user_id starts from 1

    # Find the top N most similar users (excluding the target user)
    N = 5
    top_similar_users = np.argsort(user_similarity_scores)[::-1][1:N + 1]

    # Generate movie recommendations based on the most similar users
    recommendations = []
    for user_index in top_similar_users:
        user_id_similar = user_item_matrix.index[user_index]
        recommendations.extend(df[df['user_id'] == user_id_similar]['title'].values)

    # Remove duplicates from recommendations
    recommendations = list(set(recommendations))

    # Limit to 20 recommendations
    recommendations = recommendations[:20]

    return recommendations

# Test the recommendation engines
user_ids = [1, 2, 3]
for user_id in user_ids:
    print(f"Recommendations for user {user_id}:")
    recommendations = collaborative_filtering_recommendation(user_id, df_games_users_all)
    for i, recommendation in enumerate(recommendations, 1):
        print(f"{i}. {recommendation}")

"""# Recommendation #7 - Collaborative Filtering based on 3 categories (rating, rating_score, and total_playtime) with only the top 5 of the highest playing_time for each user

The collaborative filtering recommendation model utilizes user-item interactions across multiple categories, specifically rating and total playtime, to suggest video games. Initially, the model maps string-based ratings to numerical scores and normalizes both the rating scores and total playtime. These normalized values are combined into a single score, which is used to create a user-item matrix. By calculating the cosine similarity between users, the model generates a user-user similarity matrix. The similarity scores of the target user with all other users are then used to identify the top N most similar users. Recommendations are generated based on the games enjoyed by these similar users, ensuring a diverse and personalized selection. This approach effectively integrates multiple aspects of user preferences, providing robust and accurate game recommendations.
"""

# Function to get the top 5 highest playing time for each user_id
def top_5_playtime(group):
    return group.nlargest(5, 'total_playtime')

# Apply the function to each group of user_id
top_5_playtime_all_users = df_users.groupby('user_id', group_keys=False).apply(top_5_playtime).reset_index(drop=True)

# Display the result
print(top_5_playtime_all_users.head())

# Merge df_games and df_users dataframes
df_games_users_top_5 = pd.merge(df_games[['app_id', 'title', 'genres', 'rating', 'rating_score']], top_5_playtime_all_users, on='app_id')
df_games_users_top_5 = df_games_users_top_5[['user_id', 'title', 'genres', 'rating', 'rating_score', 'total_playtime']]

# Sort by user_id
df_games_users_top_5 = df_games_users_top_5.sort_values(by='user_id')

# Display the result
print(df_games_users_top_5.head(20))

# Collaborative Filtering using User-Item Interactions with multiple categories (rating and total_playtime)
def collaborative_filtering_recommendation(user_id, df):
    # Define a mapping for string ratings to numerical scores
    rating_mapping = {
        'Overwhelmingly Positive': 5,
        'Very Positive': 4,
        'Positive': 3,
        'Mostly Positive': 2,
        'Mixed': 1,
        'Mostly Negative': 0,
        'Negative': -1,
        'Very Negative': -2,
        'Overwhelmingly Negative': -3
    }

    # Map string ratings to numerical scores
    df['rating_score'] = df['rating'].map(rating_mapping)

    # Handle cases where the rating is not in the mapping (optional)
    df['rating_score'] = pd.to_numeric(df['rating_score'], errors='coerce')
    df['rating_score'] = df['rating_score'].fillna(df['rating_score'].mean())  # Or any suitable strategy

    # Normalize 'total_playtime' and 'rating_score'
    df['total_playtime_norm'] = (df['total_playtime'] - df['total_playtime'].min()) / (df['total_playtime'].max() - df['total_playtime'].min())
    df['rating_score_norm'] = (df['rating_score'] - df['rating_score'].min()) / (df['rating_score'].max() - df['rating_score'].min())

    # Create a combined score
    df['combined_score'] = df['total_playtime_norm'] * 0.5 + df['rating_score_norm'] * 0.5

    # Create the user-item matrix using the combined score
    user_item_matrix = pd.pivot_table(df, values='combined_score', index='user_id', columns='title', fill_value=0)

    # Calculate user-user similarity matrix using cosine similarity
    user_similarity = cosine_similarity(user_item_matrix)

    # Get the similarity scores of the target user with all other users
    user_similarity_scores = user_similarity[user_id - 1]  # Adjust index if user_id starts from 1

    # Find the top N most similar users (excluding the target user)
    N = 5
    top_similar_users = np.argsort(user_similarity_scores)[::-1][1:N + 1]

    # Generate movie recommendations based on the most similar users
    recommendations = []
    for user_index in top_similar_users:
        user_id_similar = user_item_matrix.index[user_index]
        recommendations.extend(df[df['user_id'] == user_id_similar]['title'].values)

    # Remove duplicates from recommendations
    recommendations = list(set(recommendations))

    # Limit to 20 recommendations
    recommendations = recommendations[:20]

    return recommendations

# Test the recommendation engines
user_ids = [10, 20, 30]
for user_id in user_ids:
    print(f"Recommendations for user {user_id}:")
    recommendations = collaborative_filtering_recommendation(user_id, df_games_users_top_5)
    for i, recommendation in enumerate(recommendations, 1):
        print(f"{i}. {recommendation}")

"""# Recommendation #8 - Hybrid Model based on 4 categories (genres, rating, rating_score, and total_playtime) with only the top 10 of the highest playing_time for each user

The hybrid recommendation model combines user-item interactions across multiple categories, including ratings, genres, and total playtime, to suggest video games. Initially, the model maps string-based ratings to numerical scores and normalizes both the rating scores and total playtime. It then creates a TF-IDF (Term Frequency-Inverse Document Frequency) matrix for game genres. These features are combined into a single DataFrame, which is used to construct a user-item matrix. By calculating the cosine similarity between users, the model generates a user-user similarity matrix. The similarity scores of the target user with all other users are then used to identify the top N most similar users. Recommendations are generated based on the games enjoyed by these similar users, ensuring a diverse and personalized selection. This approach effectively integrates content-based filtering (using genres and ratings) with collaborative filtering (using user similarities), resulting in a robust and comprehensive recommendation system.

# Must use Kaggle due to RAM limitation
"""

# Function to get the top 10 highest playing time for each user_id
def top_10_playtime(group):
    return group.nlargest(10, 'total_playtime')

# Apply the function to each group of user_id
top_10_playtime_all_users = df_users.groupby('user_id', group_keys=False).apply(top_10_playtime).reset_index(drop=True)

# Display the result
print(top_10_playtime_all_users.head())

# Merge df_games and df_users dataframes
df_games_users_top_10 = pd.merge(df_games[['app_id', 'title', 'genres', 'rating', 'rating_score']], top_10_playtime_all_users, on='app_id')
df_games_users_top_10 = df_games_users_top_10[['user_id', 'title', 'genres', 'rating', 'rating_score', 'total_playtime']]

# Sort by user_id
df_games_users_top_10 = df_games_users_top_10.sort_values(by='user_id')

# Display the result
print(df_games_users_top_10.head(20))

# NOTE: Google Colab will not have enough RAM to run this - use Kaggle
# Hybrid Model using User-Item Interactions with multiple categories (rating, genres, and total_playtime)
def hybrid_model_recommendation(user_id, df):
    # Define a mapping for string ratings to numerical scores
    rating_mapping = {
        'Overwhelmingly Positive': 5,
        'Very Positive': 4,
        'Positive': 3,
        'Mostly Positive': 2,
        'Mixed': 1,
        'Mostly Negative': 0,
        'Negative': -1,
        'Very Negative': -2,
        'Overwhelmingly Negative': -3
    }

    # Map string ratings to numerical scores
    df['rating_score'] = df['rating'].map(rating_mapping)

    # Handle cases where the rating is not in the mapping (optional)
    df['rating_score'] = pd.to_numeric(df['rating_score'], errors='coerce')
    df['rating_score'] = df['rating_score'].fillna(df['rating_score'].mean())  # Or any suitable strategy

    # Normalize 'total_playtime' and 'rating_score'
    df['total_playtime_norm'] = (df['total_playtime'] - df['total_playtime'].min()) / (df['total_playtime'].max() - df['total_playtime'].min())
    df['rating_score_norm'] = (df['rating_score'] - df['rating_score'].min()) / (df['rating_score'].max() - df['rating_score'].min())

    # Create a TF-IDF matrix for genres
    tfidf_genres = TfidfVectorizer(stop_words='english')
    df['genres'] = df['genres'].fillna('')  # Fill NaN values with empty string
    tfidf_matrix_genres = tfidf_genres.fit_transform(df['genres'])

    # Combine normalized scores and genres into a single DataFrame
    df_combined = pd.DataFrame({
        'total_playtime_norm': df['total_playtime_norm'],
        'rating_score_norm': df['rating_score_norm']
    })

    # Add the TF-IDF matrix for genres to the combined DataFrame
    df_combined = pd.concat([df_combined, pd.DataFrame(tfidf_matrix_genres.toarray(), index=df.index)], axis=1)

    # Create the user-item matrix using the combined score
    user_item_matrix = pd.pivot_table(df_combined, index=df['user_id'], columns=df['title'], fill_value=0)

    # Calculate user-user similarity matrix using cosine similarity
    user_similarity = cosine_similarity(user_item_matrix)

    # Get the similarity scores of the target user with all other users
    user_similarity_scores = user_similarity[user_id - 1]  # Adjust index if user_id starts from 1

    # Find the top N most similar users (excluding the target user)
    N = 5
    top_similar_users = np.argsort(user_similarity_scores)[::-1][1:N + 1]

    # Generate movie recommendations based on the most similar users
    recommendations = []
    for user_index in top_similar_users:
        user_id_similar = user_item_matrix.index[user_index]
        recommendations.extend(df[df['user_id'] == user_id_similar]['title'].values)

    # Remove duplicates from recommendations
    recommendations = list(set(recommendations))

    # Limit to 20 recommendations
    recommendations = recommendations[:20]

    return recommendations

# Test the recommendation engines
user_ids = [10, 20, 30]
for user_id in user_ids:
    print(f"Recommendations for user {user_id}:")
    recommendations = hybrid_model_recommendation(user_id, df_games_users_top_10)
    for i, recommendation in enumerate(recommendations, 1):
        print(f"{i}. {recommendation}")

"""# Recommendation #9 - Popularity-Based Recommendation

This popularity-based recommendation system suggests games based on their overall engagement metrics, including total playtime, average playtime, and the number of unique players. By merging game and user data, the system calculates these metrics for each game, optionally filtering by a specific genre. A weighted popularity score is then calculated, considering total playtime, average playtime, and the number of players. Games are ranked by this score, and the top-ranked games are recommended to users. This approach provides a straightforward way to identify and suggest widely enjoyed games, potentially catering to broader user preferences.
"""

def popularity_based_recommendation(df_games, df_users, num_recommendations=10, genre=None):
    # Merge game and user data to calculate total and average playtime
    # Ensure 'genres' column is selected in the merge if it exists
    if 'genres' in df_games.columns:
        df_merged = pd.merge(df_games[['app_id', 'title', 'genres']], df_users, on='app_id')
    else:
        df_merged = pd.merge(df_games[['app_id', 'title']], df_users, on='app_id')

    df_popularity = df_merged.groupby('title').agg(
        total_playtime=('total_playtime', 'sum'),
        avg_playtime=('total_playtime', 'mean'),
        num_players=('user_id', 'nunique'),  # Number of unique players
        genres=('genres', 'first')  # Get the first genre for each title
    ).reset_index()

    # Check if 'genres' column is present and genre is provided before filtering
    if genre and 'genres' in df_popularity.columns:
        df_popularity = df_popularity[df_popularity['genres'].str.contains(genre, case=False, na=False)]

    # Calculate popularity score (you can adjust the weights)
    df_popularity['popularity_score'] = (
        df_popularity['total_playtime'] * 0.5 +  # Weight for total playtime
        df_popularity['avg_playtime'] * 0.3 +    # Weight for average playtime
        df_popularity['num_players'] * 0.2       # Weight for number of players
    )

    # Sort by popularity score and get top recommendations
    df_recommendations = df_popularity.sort_values('popularity_score', ascending=False).head(num_recommendations)

    return df_recommendations['title'].tolist()

# Example usage
recommendations = popularity_based_recommendation(df_games, df_users, genre='RPG')
print("Popular RPG Games:")
for game in recommendations:
    print(game)

recommendations = popularity_based_recommendation(df_games, df_users)  # No genre filter
print("\nOverall Popular Games:")
for game in recommendations:
    print(game)

"""# Recommendation #10 - Matrix Factorization Model

The code implements a matrix factorization technique for game recommendations using the Alternating Least Squares (ALS) algorithm from the implicit library. It first creates a user-item matrix representing user interactions (total playtime) with games. This matrix is then factorized into two lower-dimensional matrices, one representing user preferences and the other representing game characteristics. The ALS algorithm iteratively optimizes these matrices to minimize the difference between the original matrix and their product. By leveraging these learned representations, the model can predict a user's preference for games they haven't interacted with and generate personalized recommendations. The get_recommendations function utilizes the trained model to provide a specified number of game suggestions for a target user based on their predicted preferences.
"""

def build_matrix_factorization_model(df_users, df_games, num_factors=50, regularization=0.01, iterations=15):
    # Filter data for users with at least 5 games played
    df_filtered = df_users.groupby('user_id').filter(lambda x: len(x) >= 5)

    # Create a user-item matrix
    user_item_matrix = pd.pivot_table(df_filtered, values='total_playtime', index='user_id', columns='app_id', fill_value=0)

    # Convert the user-item matrix to a sparse matrix
    user_item_matrix_sparse = csr_matrix(user_item_matrix.values)

    # Initialize and train the ALS model
    model = implicit.als.AlternatingLeastSquares(factors=num_factors, regularization=regularization, iterations=iterations)
    model.fit(user_item_matrix_sparse)

    # Create app_id to index mapping
    app_id_to_index = {app_id: index for index, app_id in enumerate(user_item_matrix.columns)}
    index_to_app_id = {index: app_id for app_id, index in app_id_to_index.items()}

    # Return the sparse matrix along with other values
    # Added user_item_matrix to the return statement
    return model, app_id_to_index, index_to_app_id, user_item_matrix_sparse, user_item_matrix

def get_recommendations(user_id, model, user_item_matrix_sparse, index_to_app_id, df_games, user_item_matrix, N=10): # Add user_item_matrix as an argument
    # Find the row index corresponding to user_id in the original user-item matrix
    user_row_index = user_item_matrix.index.get_loc(user_id)

    # Get the user's row from the sparse matrix using the row index
    user_items = user_item_matrix_sparse[user_row_index]

    # Get recommendations
    recommendations_indices, _ = model.recommend(user_id, user_items, N=N)  # user_items as the user's row

    recommendations_app_ids = [index_to_app_id[index] for index in recommendations_indices]
    recommendations_titles = df_games[df_games['app_id'].isin(recommendations_app_ids)]['title'].tolist()
    return recommendations_titles


# Example usage:
# The function returns 5 values, so adjust the variable assignment accordingly
model, app_id_to_index, index_to_app_id, user_item_matrix_sparse, user_item_matrix = build_matrix_factorization_model(df_users, df_games)

# You don't need to call build_matrix_factorization_model twice in this case
# The previous line has already obtained user_item_matrix
# ... rest of your code

recommendations = get_recommendations(1, model, user_item_matrix_sparse, index_to_app_id, df_games, user_item_matrix)
print(f"Recommendations for user 1: {recommendations}")